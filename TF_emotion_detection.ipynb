{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": " TF_emotion_detection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "b2Wwti9DYZ05",
        "OnRb9xWSdvWt",
        "ow5WhdR9-vqm",
        "JKBBdhBa-m3V",
        "RzpqsYFv_5-T",
        "UQqqqPakhfzm",
        "BLPvRne5lymD"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iyp18lEI_CBb"
      },
      "source": [
        "# Text Emotion classifier with Tensorflow 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbi_vGGzO4PY",
        "outputId": "c0731f0d-68cd-40c6-ab09-134adfe81d57"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Feb 21 16:28:21 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UzDWp3jmOjg"
      },
      "source": [
        "## Download GloVe word embeddings\n",
        "We will use it to create the embedding layer so we will not have to train it in the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vurgcIbSmSaK",
        "outputId": "ca2c7f68-924e-4449-c1e0-073cdc2d7e70"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-21 16:28:22--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-02-21 16:28:23--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-02-21 16:28:23--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  2.02MB/s    in 6m 52s  \n",
            "\n",
            "2021-02-21 16:35:15 (2.00 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GylfwnLJmXua"
      },
      "source": [
        "!unzip glove*.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feKOoXEln9P9"
      },
      "source": [
        "!rm -rf glove.6B.50d.txt\n",
        "!rm -rf glove.6B.200d.txt\n",
        "!rm -rf glove.6B.300d.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrMkgEBSPMW_"
      },
      "source": [
        "## Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtpqJ8gIPCTO"
      },
      "source": [
        "!wget https://www.dropbox.com/s/607ptdakxuh5i4s/merged_training.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rYszFzWPqK6"
      },
      "source": [
        "import pickle\n",
        "\n",
        "## helper function\n",
        "def load_from_pickle(directory):\n",
        "    return pickle.load(open(directory,\"rb\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tk_4iQ-PsvY"
      },
      "source": [
        "data = load_from_pickle(directory=\"merged_training.pkl\")\n",
        "\n",
        "## using a sample\n",
        "emotions = [ \"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\n",
        "data= data[data[\"emotions\"].isin(emotions)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO1xEkykQY8D"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5ipy2VBQdz-"
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laP9BKZsQiIO"
      },
      "source": [
        "# is there any nulls\r\n",
        "\r\n",
        "data.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i-irlEkQoje"
      },
      "source": [
        "# count of nulls\r\n",
        "data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgTNDtvnQskR"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ0l96kzWN1r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_MN24JE0bUx"
      },
      "source": [
        "data.iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAM3BN_JQ9Kf"
      },
      "source": [
        "data.emotions.value_counts().plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwBcnRE3RQSC"
      },
      "source": [
        "MAX_NB_WORDS = 100000    # max no. of words for tokenizer\n",
        "MAX_SEQUENCE_LENGTH = 170 # max length of each entry (sentence), including padding\n",
        "VALIDATION_SPLIT = 0.2   # data for validation (not used in training)\n",
        "EMBEDDING_DIM = 100      # embedding dimensions for word vectors (word2vec/GloVe)\n",
        "\n",
        "train = data\n",
        "\n",
        "#labels = emotions\n",
        "y = train['emotions'].values\n",
        "comments_train = train['text']\n",
        "comments_train = list(comments_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPx48IedSTUt"
      },
      "source": [
        "y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ivFJmfwSTxS"
      },
      "source": [
        "len(comments_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSEVIbcVSiGh"
      },
      "source": [
        "max_text = (max(comments_train, key=len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSM4yn70S6xJ"
      },
      "source": [
        "len(max_text.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5rchMd6VWZs"
      },
      "source": [
        "def num_words(sentence):\n",
        "  words = sentence.split()\n",
        "  return len(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wIeBGPLTOLc"
      },
      "source": [
        "total_avg_words = sum( map(num_words, comments_train) ) / len(comments_train)\n",
        "total_avg_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeVLS2zpYGzM"
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2Wwti9DYZ05"
      },
      "source": [
        "## Text pre-proccessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2xf7CpDoo7r"
      },
      "source": [
        "import re\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X_v8bTmqgq1"
      },
      "source": [
        "def clean_text(text, remove_stopwords = True):\n",
        "    output = \"\"\n",
        "    text = str(text).replace(\"\\n\", \"\")\n",
        "    text = re.sub(r'[^\\w\\s]','',text).lower()\n",
        "    if remove_stopwords:\n",
        "        text = text.split(\" \")\n",
        "        for word in text:\n",
        "            if word not in stopwords.words(\"english\"):\n",
        "                output = output + \" \" + word\n",
        "    else:\n",
        "        output = text\n",
        "    return str(output.strip())[1:-3].replace(\"  \", \" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzRNXxd6YRZh"
      },
      "source": [
        "texts = [] \n",
        "\n",
        "for line in tqdm_notebook(comments_train, total=159571): \n",
        "    texts.append(clean_text(line))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETFF5Eb7dbcw"
      },
      "source": [
        "len(texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgRSRXEddeNU"
      },
      "source": [
        "print('Sample data:', texts[1], y[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnRb9xWSdvWt"
      },
      "source": [
        "## Tokenize the texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKTxpal-o2DS"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MToEWKBedmHU"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "print('Vocabulary size:', len(word_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jQj6lESd4EP"
      },
      "source": [
        "import json\n",
        "with open('word_index.json', 'w') as f:\n",
        "    json.dump(word_index, f)\n",
        "with open('index_word.json', 'w') as f2:\n",
        "    json.dump(tokenizer.index_word, f2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWUFMe5venYt"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "data = pad_sequences(sequences, padding = 'post', maxlen = MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wYmHzuVfJPZ"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = y[indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOczYxDefPK-"
      },
      "source": [
        "data[5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zm7BnNXqfRxm"
      },
      "source": [
        "labels[5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow5WhdR9-vqm"
      },
      "source": [
        "## One-hot encoding labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Lzq7JFcf2DA"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "lb = preprocessing.LabelBinarizer()\n",
        "lb.fit(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTffnsiMf8IU"
      },
      "source": [
        "lb.classes_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UISjirthgHbR"
      },
      "source": [
        "labels = lb.transform(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUMSiashgNaG"
      },
      "source": [
        "labels[5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuXPGlQAfYiZ"
      },
      "source": [
        "num_validation_samples = int(VALIDATION_SPLIT*data.shape[0])\n",
        "x_train = data[: -num_validation_samples]\n",
        "y_train = labels[: -num_validation_samples]\n",
        "x_val = data[-num_validation_samples: ]\n",
        "y_val = labels[-num_validation_samples: ]\n",
        "print('Number of entries in each category:')\n",
        "print('training: ', y_train.sum(axis=0))\n",
        "print('validation: ', y_val.sum(axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofk5fpxXglFW"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-QgdzHhgpIi"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I6piGBqsCtH"
      },
      "source": [
        "x_val.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7Ea8hUbtvSq"
      },
      "source": [
        "y_val.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKBBdhBa-m3V"
      },
      "source": [
        "## Create test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4-MO5HlsL7D"
      },
      "source": [
        "x_val = x_val[: -40000]\n",
        "y_val = y_val[: -40000]\n",
        "x_test = x_val[-40000: ]\n",
        "y_test = y_val[-40000: ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1xA-v7EhDdg"
      },
      "source": [
        "print('Tokenized sentences: \\n', data[10])\n",
        "print('One hot label: \\n', labels[10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzpqsYFv_5-T"
      },
      "source": [
        "## Create the embedding matrix for our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hABB26ndhIl8"
      },
      "source": [
        "embeddings_index = {}\n",
        "f = open('/content/glove.6B.100d.txt')\n",
        "print('Loading GloVe from:', '/content/glove.6B.100d.txt','...', end='')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    embeddings_index[word] = np.asarray(values[1:], dtype='float32')\n",
        "f.close()\n",
        "print(\"Done.\\n Proceeding with Embedding Matrix...\", end=\"\")\n",
        "\n",
        "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "print(\" Completed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0GL1iv6hW9h"
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQqqqPakhfzm"
      },
      "source": [
        "## Create the model (function API)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "874LqeJFtTkg"
      },
      "source": [
        "from tensorflow.keras import regularizers, initializers, optimizers, callbacks\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Rcmr2-wkGqw"
      },
      "source": [
        "MAX_SEQUENCE_LENGTH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiGxU1NthiQ9"
      },
      "source": [
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedding_layer = Embedding(len(word_index) + 1,\n",
        "                           EMBEDDING_DIM,\n",
        "                           weights = [embedding_matrix],\n",
        "                           input_length = MAX_SEQUENCE_LENGTH,\n",
        "                           trainable=False,\n",
        "                           name = 'embeddings')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "x = LSTM(60, return_sequences=True,name='lstm_layer')(embedded_sequences)\n",
        "x = GlobalMaxPool1D()(x)\n",
        "x = Dropout(0.1)(x)\n",
        "x = Dense(50, activation=\"relu\")(x)\n",
        "x = Dropout(0.1)(x)\n",
        "preds = Dense(6, activation=\"softmax\")(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHp7DbdJhuMq"
      },
      "source": [
        "model = Model(sequence_input, preds)\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baScL0nUh2az"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.utils.plot_model(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whiw6yd6iA-Y"
      },
      "source": [
        "print('Training progress:')\n",
        "history = model.fit(x_train, y_train, epochs = 10, batch_size=128, validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7pVZVQOibnL"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(loss)+1)\n",
        "plt.plot(epochs, loss, label='Training loss')\n",
        "plt.plot(epochs, val_loss, label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZwhCBSTi0g2"
      },
      "source": [
        "accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "plt.plot(epochs, accuracy, label='Training accuracy')\n",
        "plt.plot(epochs, val_accuracy, label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g5O1Nap4B0b"
      },
      "source": [
        "print(\"Accuracy in the test set:\")\n",
        "model.evaluate(x_test, y_test)[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLPvRne5lymD"
      },
      "source": [
        "## Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCpK7qyol1dU"
      },
      "source": [
        "samples = ['i feel like, i do not know...', 'love you woman', 'that is so funny', 'mamma, i just killed a man', 'i want to ride my bicycle', 'im alone i feel awful', 'i beleive that i am much more sensitive to oth...']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHoN-zl1vXVx"
      },
      "source": [
        "cleaned_samples = []\n",
        "for sentence in samples:\n",
        "  print(sentence)\n",
        "  cleaned = clean_text(sentence)\n",
        "  print(cleaned)\n",
        "  cleaned_samples.append(cleaned)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TErUPivwgEK"
      },
      "source": [
        "tokenized_seq = tokenizer.texts_to_sequences(cleaned_samples)\n",
        "padded_seq =  pad_sequences(tokenized_seq, padding = 'post', maxlen = MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z863mKHHysRt"
      },
      "source": [
        "int2label = {\n",
        "    0: 'anger',\n",
        "    1: 'fear',\n",
        "    2: 'joy',\n",
        "    3: 'love',\n",
        "    4: 'sadness',\n",
        "    5: 'surprise'\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5zLcDjCli8R"
      },
      "source": [
        "predictions = model.predict(padded_seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uXrH6FSxsgN"
      },
      "source": [
        "for i, prediction in enumerate(predictions):\n",
        "  print(samples[i] +\" => \" +int2label[(np.argmax(prediction))])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}